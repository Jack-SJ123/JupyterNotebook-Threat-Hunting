{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "150a142e",
   "metadata": {},
   "source": [
    "Threat Hunting — Web Scraping\n",
    "\n",
    "Web scraping is the process of automatically extracting data from websites. This typically involves sending HTTP requests to a website’s server, parsing the HTML or XML content of the response, and then extracting the relevant data using specific patterns or rules. This data is then stored in a structured format such as a database or spreadsheet where it can be analyzed or used. \n",
    "To perform web scraping in Python we will use Beautiful Soup, which is a Python library for web scraping that provides a simple and intuitive way to parse HTML and XML documents. It is widely used in web scraping because it can handle various parsing tasks with minimal code.\n",
    "\n",
    "To demonstrate web scraping we are going to keep track of high severity vulnerabilities (vulnerabilities with a CVSS base score of 7.0–10.0), very common in the world of threat intelligence. These vulnerabilities can often lead to Remote Code Execution (RCE) and provide an attacker initial access to a system. \n",
    "The US-based Cybersecurity and Infrastructure Security Agency (CISA) maintains a weekly summary of vulnerabilities which it posts as a bulletin on its website. This page divides vulnerabilities into four tables based on severity; High Vulnerabilities, Medium Vulnerabilities, Low Vulnerabilities, and Severity Not Yet Assigned. We will be focusing on the High Vulnerabilities table, which is the first table on the page. To capture this table we first need to make a request to download this page and use Beautiful Soup’s html.parser to parse the HTML returned by our request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b160edce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vulnerability Summary for the Week of July 21, 2025 | CISA>\n",
      "=> CISA vulnerability summary page parsed and file <CISA Vulnerabilities - July 21, 2025.csv> created.\n"
     ]
    }
   ],
   "source": [
    "# This script is to extract high vulnerabilities found on CISA’s security bulletin site (https://www.cisa.gov/news-events/bulletins).\n",
    "# Copy the URL into the CISA_URL variable and run the script, you will get a list of high vulnerabilities for the week.\n",
    "\n",
    "import requests, csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Download the page and parse the page\n",
    "CISA_URL = \"https://www.cisa.gov/news-events/bulletins/sb25-209\"        # Change the url to the one you want to investigate\n",
    "page = requests.get(CISA_URL)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# define variables\n",
    "Page_Title = soup.title.string\n",
    "a = soup.title.string.split(\"of\")\n",
    "b = a[1].split(\"|\")\n",
    "Filename = \"CISA Vulnerabilities - \" + b[0].strip() + \".csv\"\n",
    "\n",
    "# capture high vulnerabilities table\n",
    "table = soup.find(\"table\")\n",
    "table_body = table.find(\"tbody\")\n",
    "rows = table_body.find_all(\"tr\")        # find all the table rows using the tr (table row) tag (<tr>).\n",
    "\n",
    "vulns = []      # create vulnerabilities list\n",
    "\n",
    "# use a loop to iterate through all the rows stored in the rows variable and finds the table data stored in the HTML td (table data) tag (<td>)\n",
    "for row in rows:\n",
    "    cols = [x for x in row.find_all(\"td\")]\n",
    "\n",
    "    # get the text property and use the Python string method .strip() to remove whitespaces\n",
    "    product, vendor = cols[0].text.split(\"--\")\n",
    "    description = cols[1].text.strip()\n",
    "    published = cols[2].text.strip()\n",
    "    cvss = cols[3].text.strip()\n",
    "    cve = cols[4].find(\"a\").text.strip()        # find the anchor tag (<a>) and extract the text property\n",
    "    reference = cols[4].find(\"a\").get(\"href\")   # extract the link at this anchor tag (stored in its href attribute)\n",
    "\n",
    "    # create a Python dictionary object vuln to store the data\n",
    "    vuln = {\n",
    "        \"product\": product.strip(),\n",
    "        \"vendor\": vendor.strip(),\n",
    "        \"description\": description,\n",
    "        \"published\": published,\n",
    "        \"cvss\": cvss,\n",
    "        \"cve\": cve,\n",
    "        \"reference\": reference\n",
    "    }\n",
    "    vulns.append(vuln)\n",
    "\n",
    "# define csv file header row\n",
    "header_row = [\"Product\", \"Vendor\", \"Description\", \"Published\", \"CVSS\", \"CVE\", \"Reference\"]\n",
    "\n",
    "# create a csv file as output to store all the data parsed, this file can transfer to relevant parties for system patching planning\n",
    "with open(Filename, \"w\", encoding='UTF8', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header_row)\n",
    "\n",
    "    for vuln in vulns:\n",
    "        data_row = [vuln['product'], vuln['vendor'], vuln['description'], vuln['published'], vuln['cvss'], vuln['cve'], vuln['reference']]\n",
    "        writer.writerow(data_row)\n",
    "\n",
    "print(f\"<{Page_Title}>\\n=> CISA vulnerability summary page parsed and file <{Filename}> created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
